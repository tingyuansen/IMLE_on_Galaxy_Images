{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# import packages\n",
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib import gridspec\n",
    "\n",
    "import torch\n",
    "from torch.nn import Linear, LeakyReLU, MSELoss, Sequential\n",
    "from torch.optim import Adam\n",
    "\n",
    "from kymatio import Scattering1D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define plot properties\n",
    "from cycler import cycler\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "from matplotlib import rcParams\n",
    "from matplotlib import rc\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "def rgb(r,g,b):\n",
    "    return (float(r)/256.,float(g)/256.,float(b)/256.)\n",
    "\n",
    "cb2 = [rgb(31,120,180), rgb(255,127,0), rgb(51,160,44), rgb(227,26,28), \\\n",
    "       rgb(166,206,227), rgb(253,191,111), rgb(178,223,138), rgb(251,154,153)]\n",
    "\n",
    "rcParams['figure.figsize'] = (9,7.5)\n",
    "#rcParams['figure.dpi'] = 300\n",
    "\n",
    "rcParams['lines.linewidth'] = 1\n",
    "\n",
    "rcParams['axes.prop_cycle'] = cycler('color', cb2)\n",
    "rcParams['axes.facecolor'] = 'white'\n",
    "rcParams['axes.grid'] = False\n",
    "\n",
    "rcParams['patch.facecolor'] = cb2[0]\n",
    "rcParams['patch.edgecolor'] = 'white'\n",
    "\n",
    "rcParams['font.size'] = 23\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Calculate scattering coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "from kymatio import Scattering1D\n",
    "import kymatio\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim\n",
    "import torch\n",
    "import torch.utils.data as utils\n",
    "\n",
    "import time\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "#=========================================================================================================\n",
    "# load light curves\n",
    "real_spec = np.load(\"../light_curve.npy\")[:10,:]\n",
    "print(real_spec.shape)\n",
    "\n",
    "### change the amplitude\n",
    "#real_spec = real_spec*2.\n",
    "\n",
    "## mix two modes\n",
    "#real_spec = (real_spec[:,:] + real_spec[::-1,:])/2.\n",
    "\n",
    "\n",
    "#================================================================================================\n",
    "# define wavelet scattering hyperparameters\n",
    "J = 6\n",
    "Q = 8\n",
    "T = real_spec.shape[1]\n",
    "\n",
    "# convert into torch variable\n",
    "x = torch.from_numpy(real_spec[:,:T]).type(torch.FloatTensor)\n",
    "print(x.shape)\n",
    "\n",
    "# define wavelet scattering\n",
    "scattering = Scattering1D(J, T, Q)\n",
    "\n",
    "#================================================================================================\n",
    "# perform wavelet scattering\n",
    "Sx_all = scattering.forward(x)\n",
    "\n",
    "# calculate invariate representation\n",
    "Sx_all = torch.mean(Sx_all, dim=-1)\n",
    "\n",
    "# normalize wrt to the first coefficient\n",
    "for i in range(Sx_all.shape[0]):\n",
    "    Sx_all[i,:] = Sx_all[i,:]/np.abs(Sx_all[i,0])\n",
    "    \n",
    "# take log to normalize the coefficient better\n",
    "Sx_all = torch.log10(Sx_all[:,1:])\n",
    "print(Sx_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sx_all_1 = np.copy(Sx_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sx_all = Sx_all.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Sx_all_1[0,:])\n",
    "plt.plot(Sx_all_1[0,:])\n",
    "plt.plot(Sx_all[0,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Check wavelet scattering coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load scattering coefficients\n",
    "Sx = np.load(\"../Sx_all.npy\")\n",
    "plt.hist(Sx.ravel(), bins=100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Sample log probablity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import numpy as np\n",
    "from astropy.io import fits\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import distributions\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.autograd import Variable\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "\n",
    "#========================================================================================================\n",
    "# read scattering coefficents\n",
    "#y_tr = np.load(\"../Sx_all_mixed.npy\")\n",
    "\n",
    "# restore Lomb Scargle coefficients\n",
    "temp = np.load(\"../g_lomb_scargle.npz\")\n",
    "y_tr = temp[\"power_array\"][:,:50][:,::3]\n",
    "\n",
    "# convert into torch\n",
    "y_tr = torch.from_numpy(y_tr).type(torch.FloatTensor)\n",
    "\n",
    "\n",
    "#=======================================================================================================\n",
    "# In [2]:\n",
    "# define normalizing flow\n",
    "class RealNVP(nn.Module):\n",
    "    def __init__(self, nets, nett, mask, prior):\n",
    "        super(RealNVP, self).__init__()\n",
    "\n",
    "        self.prior = prior\n",
    "        self.mask = nn.Parameter(mask, requires_grad=False)\n",
    "        self.t = torch.nn.ModuleList([nett() for _ in range(len(masks))])\n",
    "        self.s = torch.nn.ModuleList([nets() for _ in range(len(masks))])\n",
    "\n",
    "    def g(self, z):\n",
    "        x = z\n",
    "        for i in range(len(self.t)):\n",
    "            x_ = x*self.mask[i]\n",
    "            s = self.s[i](x_)*(1 - self.mask[i])\n",
    "            t = self.t[i](x_)*(1 - self.mask[i])\n",
    "            x = x_ + (1 - self.mask[i]) * (x * torch.exp(s) + t)\n",
    "        return x\n",
    "\n",
    "    def f(self, x):\n",
    "        log_det_J, z = x.new_zeros(x.shape[0]), x\n",
    "        for i in reversed(range(len(self.t))):\n",
    "            z_ = self.mask[i] * z\n",
    "            s = self.s[i](z_) * (1-self.mask[i])\n",
    "            t = self.t[i](z_) * (1-self.mask[i])\n",
    "            z = (1 - self.mask[i]) * (z - t) * torch.exp(-s) + z_\n",
    "            log_det_J -= s.sum(dim=1)\n",
    "        return z, log_det_J\n",
    "\n",
    "    def log_prob(self,x):\n",
    "        z, logp = self.f(x)\n",
    "        return self.prior.log_prob(z) + logp\n",
    "\n",
    "    def sample(self, z):\n",
    "        x = self.g(z)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "#==================================================================================\n",
    "# restore models\n",
    "flow = torch.load(\"../flow_final_g_short.pt\", map_location=lambda storage, loc: storage) # load in cpu\n",
    "flow.eval()\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------\n",
    "# sample results\n",
    "log_prob_x = flow.log_prob(y_tr).detach().numpy()\n",
    "\n",
    "# save results\n",
    "np.savez(\"../real_nvp_results_light_curve_normal.npz\",\\\n",
    "         log_prob_x = log_prob_x)\n",
    "\n",
    "print(log_prob_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = np.load(\"../real_nvp_results_light_curve_validation.npz\")\n",
    "plt.hist(temp[\"log_prob_x\"], bins=50, label=\"mixed\", alpha=0.5, range=[0,300]);\n",
    "\n",
    "temp = np.load(\"../real_nvp_results_light_curve_normal.npz\")\n",
    "plt.hist(temp[\"log_prob_x\"], bins=50, label=\"normal\", alpha=0.5, range=[0,300]);\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.xlabel(\"log probability\")\n",
    "plt.ylabel(\"# light curves\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Processes inputation.\n",
    "\n",
    "> Run with GPy. First a simple example.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import GPy\n",
    "\n",
    "# make mock data\n",
    "X = np.random.uniform(-3.,3.,(20,1))\n",
    "Y = np.sin(X) + np.random.randn(20,1)*0.05\n",
    "\n",
    "# define kernel\n",
    "kernel = GPy.kern.RBF(input_dim=1, variance=1., lengthscale=1.)\n",
    "m = GPy.models.GPRegression(X,Y,kernel)\n",
    "\n",
    "# optimize\n",
    "m.optimize(messages=True)\n",
    "m.optimize_restarts(num_restarts = 10)\n",
    "\n",
    "# display results\n",
    "fig = m.plot(plot_density=True)\n",
    "GPy.plotting.show(fig, filename='basic_gp_regression_density_notebook_optimized')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Make prediction with the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X,Y)\n",
    "\n",
    "X_array = np.linspace(-4,4,100)\n",
    "X_array = X_array.reshape(X_array.size,1)\n",
    "\n",
    "Y_predict = np.array(m.predict(X_array))[0,:,:]\n",
    "plt.plot(X_array,Y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Try with real light curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import GPy\n",
    "\n",
    "# load light curve\n",
    "temp = np.load(\"../light_curve.npz\")\n",
    "light_curve = temp[\"light_curve\"]\n",
    "t_array = temp[\"t_array\"]\n",
    "\n",
    "# extract a single light cure\n",
    "X = t_array[0,:]\n",
    "X = X.reshape(X.size,-1)\n",
    "Y = light_curve[0,:]\n",
    "Y = Y.reshape(Y.size,-1)\n",
    "\n",
    "# define kernel\n",
    "kernel = GPy.kern.RBF(input_dim=1, variance=1., lengthscale=1.)\n",
    "m = GPy.models.GPRegression(X[::10],Y[::10],kernel)\n",
    "\n",
    "# optimize\n",
    "m.optimize(messages=True)\n",
    "m.optimize_restarts(num_restarts = 10)\n",
    "\n",
    "# display results\n",
    "fig = m.plot(plot_density=True)\n",
    "GPy.plotting.show(fig, filename='basic_gp_regression_density_notebook_optimized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(t_array[0,:],light_curve[0,:], color=cb2[1])\n",
    "\n",
    "X_array = t_array[0,:]\n",
    "X_array = X_array.reshape(X_array.size,1)\n",
    "\n",
    "Y_predict = np.array(m.predict(X_array))[0,:,0]\n",
    "Y_std = np.sqrt(np.array(m.predict(X_array))[1,:,0])\n",
    "\n",
    "plt.plot(X_array[:,0],Y_predict, color=\"black\")\n",
    "plt.fill_between(X_array[:,0], Y_predict-Y_std, Y_predict+Y_std, color=cb2[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SDSS DR14 QSO.\n",
    "\n",
    "> Read data files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## all flies\n",
    "import os\n",
    "list_files = os.listdir(\"../qso\")\n",
    "\n",
    "# initiate array\n",
    "mjd_g = []\n",
    "g_array = []\n",
    "mjd_r = []\n",
    "r_array = []\n",
    "list_files_store = []\n",
    "\n",
    "#-------------------------------------------------------------------------------------\n",
    "# loop over all files\n",
    "for i in range(len(list_files)):\n",
    "        \n",
    "    try:\n",
    "        # read data\n",
    "        data = np.loadtxt(\"../qso/\" + list_files[i])\n",
    "        mjd_g_temp = data[:,2]\n",
    "        g_temp = data[:,3]\n",
    "        mjd_r_temp = data[:,0]\n",
    "        r_temp = data[:,1]\n",
    "    \n",
    "        # cull empty entries and sort by time\n",
    "        choose = mjd_g_temp > 10.\n",
    "        mjd_g_temp = mjd_g_temp[choose]\n",
    "        g_temp = g_temp[choose]\n",
    "        choose = np.argsort(mjd_g_temp)\n",
    "        mjd_g_temp = mjd_g_temp[choose]\n",
    "        g_temp = g_temp[choose]\n",
    "    \n",
    "        choose = mjd_r_temp > 10.\n",
    "        mjd_r_temp = mjd_r_temp[choose]\n",
    "        r_temp = r_temp[choose]\n",
    "        choose = np.argsort(mjd_r_temp)\n",
    "        mjd_r_temp = mjd_r_temp[choose]\n",
    "        r_temp = r_temp[choose]\n",
    "\n",
    "        # record results\n",
    "        mjd_g.append(mjd_g_temp)\n",
    "        g_array.append(g_temp)\n",
    "        mjd_r.append(mjd_r_temp)\n",
    "        r_array.append(r_temp)\n",
    "        list_files_store.append(list_files[i])\n",
    "        \n",
    "    except:\n",
    "        print(i)\n",
    "\n",
    "# convert to numpy array\n",
    "mjd_g = np.array(mjd_g)\n",
    "g_array = np.array(g_array)\n",
    "mjd_r = np.array(mjd_r)\n",
    "r_array = np.array(r_array)\n",
    "list_files = np.array(list_files_store)\n",
    "\n",
    "#-------------------------------------------------------------------------------------\n",
    "# save results\n",
    "np.savez(\"../SDSS_DR14_qso.npz\",\\\n",
    "         mjd_g = mjd_g,\\\n",
    "         g_array = g_array,\\\n",
    "         mjd_r = mjd_r,\\\n",
    "         r_array = r_array,\\\n",
    "         list_files = list_files)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Investigate the time stamp range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time step range\n",
    "mjd_g_range = np.array([mjd_g[i][-1]-mjd_g[i][0] for i in range(mjd_g.size)])\n",
    "mjd_r_range = np.array([mjd_r[i][-1]-mjd_r[i][0] for i in range(mjd_g.size)])\n",
    "\n",
    "plt.hist(mjd_g_range, bins=100, range=[400,500]);\n",
    "plt.hist(mjd_r_range, bins=100, range=[400,500]);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time step recorded\n",
    "mjd_g_range = np.array([len(mjd_g[i]) for i in range(mjd_g.size)])\n",
    "mjd_r_range = np.array([len(mjd_r[i]) for i in range(mjd_g.size)])\n",
    "\n",
    "plt.hist(mjd_g_range, bins=100);\n",
    "plt.hist(mjd_r_range, bins=100);\n",
    "\n",
    "#print(np.where(mjd_g_range > 200))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Try Gaussian Processes on light curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import GPy\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "#==================================================================================\n",
    "### restore mock grid ###\n",
    "temp = np.load(\"../SDSS_DR14_qso_mock.npz\", allow_pickle=True)\n",
    "mjd_g = temp[\"t_array\"]\n",
    "g_array = temp[\"light_curve\"]\n",
    "\n",
    "\n",
    "#==================================================================================\n",
    "# choose one object\n",
    "ind_choose = 0\n",
    "\n",
    "# extract a single light cure\n",
    "X = (mjd_g[ind_choose] - mjd_g[ind_choose][0])\n",
    "X = X.reshape(X.size,-1)\n",
    "Y = g_array[ind_choose]\n",
    "Y = Y.reshape(Y.size,-1)\n",
    "\n",
    "# define kernel\n",
    "k0 = GPy.kern.Matern32(1) \n",
    "k1 = GPy.kern.Matern32(1) \n",
    "k2 = GPy.kern.Matern32(1) \n",
    "\n",
    "kernel = k0 + k1 + k2 \n",
    "m = GPy.models.GPRegression(X, Y, kernel, normalizer=True)\n",
    "#print(m.kern)\n",
    "\n",
    "#-------------------------------------------------------------------------------------\n",
    "# set range parameters\n",
    "# m.kern.Mat32.lengthscale.constrain_bounded(0.1,1)\n",
    "# m.kern.Mat32.variance.constrain_bounded(1e-10,1e-5)\n",
    "\n",
    "# m.kern.Mat32_1.lengthscale.constrain_bounded(1,10)\n",
    "# m.kern.Mat32_1.variance.constrain_bounded(1e-10,1e-5)\n",
    "\n",
    "# m.kern.Mat32_2.lengthscale.constrain_bounded(10,100)\n",
    "# m.kern.Mat32_2.variance.constrain_bounded(1e-10,1.)\n",
    "\n",
    "# Fix the noise variance to known value \n",
    "#m.Gaussian_noise.variance = 1e-3**2\n",
    "#m.Gaussian_noise.variance.fix()\n",
    "\n",
    "# optimize\n",
    "m.optimize(messages=True)\n",
    "m.optimize_restarts(num_restarts = 10)\n",
    "print(m.kern)\n",
    "\n",
    "#-------------------------------------------------------------------------------------\n",
    "# extract parameters\n",
    "lengthscale_array = np.array([m.kern.Mat32.lengthscale[0],\\\n",
    "                         m.kern.Mat32_1.lengthscale[0],\\\n",
    "                         m.kern.Mat32_2.lengthscale[0]])\n",
    "variance_array = np.array([m.kern.Mat32.variance[0],\\\n",
    "                         m.kern.Mat32_1.variance[0],\\\n",
    "                         m.kern.Mat32_2.variance[0]])\n",
    "\n",
    "# sort by lengthscale\n",
    "length_sort = np.argsort(lengthscale_array)\n",
    "lengthscale_array = lengthscale_array[length_sort]\n",
    "variance_array = variance_array[length_sort]\n",
    "\n",
    "# combine all parameters\n",
    "kernel_param = np.concatenate([lengthscale_array,variance_array])\n",
    "print(kernel_param)\n",
    "\n",
    "#-------------------------------------------------------------------------------------\n",
    "# display results\n",
    "fig = m.plot(plot_density=True)\n",
    "GPy.plotting.show(fig, filename='basic_gp_regression_density_notebook_optimized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set grid to interpolate into\n",
    "X_array = np.arange(5120)*0.1\n",
    "X_array = X_array.reshape(X_array.size,1)\n",
    "\n",
    "# make prediction\n",
    "Y_predict = np.array(m.predict(X_array))[0,:,0]\n",
    "Y_std = np.sqrt(np.array(m.predict(X_array))[1,:,0])\n",
    "\n",
    "# original data\n",
    "plt.scatter(X.ravel(), Y.ravel(), color=cb2[1])\n",
    "\n",
    "# prediction\n",
    "plt.plot(X_array[:,0],Y_predict, color=\"black\")\n",
    "plt.fill_between(X_array[:,0], Y_predict-Y_std, Y_predict+Y_std, color=cb2[0], alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Check time step gaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restore grid\n",
    "temp = np.load(\"../SDSS_DR14_qso.npz\", allow_pickle=True)\n",
    "mjd_g = temp[\"mjd_g\"]\n",
    "mjd_gap = []\n",
    "for i in range(mjd_g.size):\n",
    "    mjd_gap.extend(mjd_g[i][1:]-mjd_g[i][:-1])\n",
    "mjd_gap = np.array(mjd_gap)\n",
    "print(mjd_gap.size)\n",
    "plt.hist(mjd_gap, bins=100, range=[0,0.01])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Gaussian Processes in batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian Processes package\n",
    "import GPy\n",
    "import time\n",
    "from multiprocessing import Pool\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# set number of threads per CPU\n",
    "os.environ['OMP_NUM_THREADS']='{:d}'.format(1)\n",
    "\n",
    "#-------------------------------------------------------------------------------------\n",
    "# restore grid\n",
    "# temp = np.load(\"../SDSS_DR14_qso.npz\", allow_pickle=True)\n",
    "# mjd_g = temp[\"mjd_g\"]\n",
    "# g_array = temp[\"g_array\"]\n",
    "\n",
    "### restore mock grid ###\n",
    "temp = np.load(\"../SDSS_DR14_qso_mock.npz\", allow_pickle=True)\n",
    "mjd_g = temp[\"t_array\"]\n",
    "g_array = temp[\"light_curve\"]\n",
    "\n",
    "# set grid to interpolate into\n",
    "# X_array = np.arange(5120)*0.1\n",
    "# X_array = X_array.reshape(X_array.size,1)\n",
    "\n",
    "#-------------------------------------------------------------------------------------\n",
    "# interpolate with GP\n",
    "def GP_interp(ind_choose):\n",
    "\n",
    "    # extract a single light cure\n",
    "    X = (mjd_g[ind_choose] - mjd_g[ind_choose][0])\n",
    "    X = X.reshape(X.size,-1)\n",
    "    Y = g_array[ind_choose]\n",
    "    Y = Y.reshape(Y.size,-1)\n",
    "\n",
    "    # define kernel\n",
    "    k0 = GPy.kern.Matern32(1) \n",
    "    k1 = GPy.kern.Matern32(1) \n",
    "    k2 = GPy.kern.Matern32(1) \n",
    "    kernel = k0 + k1 + k2 \n",
    "    \n",
    "#-------------------------------------------------------------------------------------\n",
    "    # define regression\n",
    "    m = GPy.models.GPRegression(X, Y, kernel, normalizer=True)\n",
    "\n",
    "    # set range parameters\n",
    "#     m.kern.Mat32.lengthscale.constrain_bounded(0.1,1)\n",
    "#     m.kern.Mat32.variance.constrain_bounded(1e-10,1e-5)\n",
    "\n",
    "#     m.kern.Mat32_1.lengthscale.constrain_bounded(1,10)\n",
    "#     m.kern.Mat32_1.variance.constrain_bounded(1e-10,1e-5)\n",
    "\n",
    "#     m.kern.Mat32_2.lengthscale.constrain_bounded(10,100)\n",
    "#     m.kern.Mat32_2.variance.constrain_bounded(1e-10,1.)\n",
    "\n",
    "    # fix the noise variance to known value \n",
    "#     m.Gaussian_noise.variance = 1e-2**2\n",
    "#     m.Gaussian_noise.variance.fix()\n",
    "\n",
    "#-------------------------------------------------------------------------------------\n",
    "    # optimize\n",
    "    m.optimize(messages=True)\n",
    "    m.optimize_restarts(num_restarts = 10)\n",
    "\n",
    "    # make prediction\n",
    "    #Y_predict = np.array(m.predict(X_array))[0,:,0]\n",
    "    \n",
    "#-------------------------------------------------------------------------------------\n",
    "    # extract parameters\n",
    "    lengthscale_array = np.array([m.kern.Mat32.lengthscale[0],\\\n",
    "                                  m.kern.Mat32_1.lengthscale[0],\\\n",
    "                                  m.kern.Mat32_2.lengthscale[0]])\n",
    "    variance_array = np.array([m.kern.Mat32.variance[0],\\\n",
    "                               m.kern.Mat32_1.variance[0],\\\n",
    "                               m.kern.Mat32_2.variance[0]])\n",
    "\n",
    "    # sort by lengthscale\n",
    "    length_sort = np.argsort(lengthscale_array)\n",
    "    lengthscale_array = lengthscale_array[length_sort]\n",
    "    variance_array = variance_array[length_sort]\n",
    "\n",
    "    # combine all parameters\n",
    "    Y_predict = np.concatenate([lengthscale_array,variance_array])\n",
    "\n",
    "#-------------------------------------------------------------------------------------\n",
    "    # return prediction\n",
    "    return Y_predict\n",
    "\n",
    "\n",
    "#=====================================================================================\n",
    "# number of CPU to run in parallel\n",
    "num_CPU = 4\n",
    "pool = Pool(num_CPU)\n",
    "start_time = time.time()\n",
    "Y_predict_array = np.array(pool.map(GP_interp,range(mjd_g.size)))\n",
    "print(time.time()-start_time)\n",
    "    \n",
    "# save results\n",
    "np.save(\"../kernel_param_mock.npy\", np.array(Y_predict_array))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Plot training loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = np.load(\"../loss_results.npz\")\n",
    "plt.plot(temp[\"loss_array\"])\n",
    "plt.ylim([-500,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lomb Sargles as representations.\n",
    "\n",
    "> Using Lomb Scargles as a metric.\n",
    "\n",
    "Cannot use the interpolated version, because the Gaussian kernel period will imprint on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.timeseries import LombScargle\n",
    "from scipy import interpolate\n",
    "\n",
    "# choose a frequency grid to interpolate into\n",
    "frequency_interp = np.arange(121)/120.*0.6\n",
    "\n",
    "\n",
    "#====================================================================================\n",
    "# restore grid\n",
    "#temp = np.load(\"../SDSS_DR14_qso.npz\", allow_pickle=True)\n",
    "#mjd_g = temp[\"mjd_g\"]\n",
    "#g_array = temp[\"g_array\"]\n",
    "\n",
    "temp = np.load(\"../SDSS_DR14_qso_mock_3000.npz\", allow_pickle=True)\n",
    "mjd_g = temp[\"t_array\"]\n",
    "g_array = temp[\"light_curve\"]\n",
    "\n",
    "\n",
    "#====================================================================================\n",
    "# initiate result arrays\n",
    "power_array = []\n",
    "\n",
    "# loop over all objects\n",
    "for i in range(g_array.size):\n",
    "    if i % 100 == 0:\n",
    "        print(i)\n",
    "        \n",
    "    frequency, power = LombScargle(mjd_g[i], g_array[i]).autopower(method='slow')\n",
    "    f_power = interpolate.interp1d(frequency, power, bounds_error=False, fill_value=0.)\n",
    "    power_array.append(f_power(frequency_interp))\n",
    "\n",
    "#-------------------------------------------------------------------------------------\n",
    "# convert to numpy array\n",
    "power_array = np.array(power_array)\n",
    "\n",
    "# save results\n",
    "np.savez(\"../g_lomb_scargle_3000.npz\",\\\n",
    "         frequency_interp = frequency_interp,\\\n",
    "         power_array = power_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = np.load(\"../g_lomb_scargle_3000.npz\")\n",
    "power_array = temp[\"power_array\"]\n",
    "plt.plot(frequency_interp[:50][::3], power_array[0,:50][::3])\n",
    "for i in range(1000):\n",
    "    plt.plot(frequency_interp[:50][::3], power_array[i,:50][::3], color=cb2[0], alpha=0.1)\n",
    "\n",
    "temp = np.load(\"../g_lomb_scargle.npz\")\n",
    "power_array = temp[\"power_array\"]\n",
    "plt.plot(frequency_interp[:50][::3], power_array[0,:50][::3])\n",
    "for i in range(1000):\n",
    "    plt.plot(frequency_interp[:50][::3], power_array[i,:50][::3], color=cb2[1], alpha=0.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Check mock SDSS qso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restore grid\n",
    "temp = np.load(\"../SDSS_DR14_qso_mock.npz\", allow_pickle=True)\n",
    "mjd_g = temp[\"t_array\"]\n",
    "g_array = temp[\"light_curve\"]\n",
    "\n",
    "ind_choose = 90\n",
    "print(mjd_g[ind_choose].size)\n",
    "plt.scatter(mjd_g[ind_choose],g_array[ind_choose])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
